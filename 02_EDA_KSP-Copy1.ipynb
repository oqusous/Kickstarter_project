{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Feature Selection, Engineering and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cleaned data and make it into a pandas Data Frame\n",
    "with open('json_/cleaned_data_1.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensure Data Was Imported Correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9121, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9121 entries, 0 to 999\n",
      "Data columns (total 25 columns):\n",
      "backers_count               9121 non-null int64\n",
      "converted_pledged_amount    9121 non-null int64\n",
      "country                     9121 non-null object\n",
      "created_at                  9121 non-null datetime64[ns]\n",
      "currency                    9121 non-null object\n",
      "current_currency            9121 non-null object\n",
      "deadline                    9121 non-null int64\n",
      "disable_communication       9121 non-null bool\n",
      "fx_rate                     9121 non-null float64\n",
      "goal                        9121 non-null float64\n",
      "id                          9121 non-null int64\n",
      "is_starrable                9121 non-null bool\n",
      "launched_at                 9121 non-null datetime64[ns]\n",
      "name                        9121 non-null object\n",
      "pledged                     9121 non-null float64\n",
      "spotlight                   9121 non-null bool\n",
      "staff_pick                  9121 non-null bool\n",
      "state                       9121 non-null object\n",
      "state_changed_at            9121 non-null datetime64[ns]\n",
      "static_usd_rate             9121 non-null float64\n",
      "urls                        9121 non-null object\n",
      "usd_pledged                 9121 non-null float64\n",
      "cat_name                    9121 non-null object\n",
      "cat_slug                    9121 non-null object\n",
      "loc_state                   9121 non-null object\n",
      "dtypes: bool(4), datetime64[ns](3), float64(5), int64(4), object(9)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backers_count</th>\n",
       "      <th>converted_pledged_amount</th>\n",
       "      <th>country</th>\n",
       "      <th>created_at</th>\n",
       "      <th>currency</th>\n",
       "      <th>current_currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>disable_communication</th>\n",
       "      <th>fx_rate</th>\n",
       "      <th>goal</th>\n",
       "      <th>...</th>\n",
       "      <th>spotlight</th>\n",
       "      <th>staff_pick</th>\n",
       "      <th>state</th>\n",
       "      <th>state_changed_at</th>\n",
       "      <th>static_usd_rate</th>\n",
       "      <th>urls</th>\n",
       "      <th>usd_pledged</th>\n",
       "      <th>cat_name</th>\n",
       "      <th>cat_slug</th>\n",
       "      <th>loc_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1847</td>\n",
       "      <td>GB</td>\n",
       "      <td>2015-04-03</td>\n",
       "      <td>GBP</td>\n",
       "      <td>USD</td>\n",
       "      <td>1430956800000</td>\n",
       "      <td>False</td>\n",
       "      <td>1.30399</td>\n",
       "      <td>7000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>canceled</td>\n",
       "      <td>2015-04-19</td>\n",
       "      <td>1.491538</td>\n",
       "      <td>{\"web\":{\"project\":\"https://www.kickstarter.com...</td>\n",
       "      <td>1842.049134</td>\n",
       "      <td>Ready-to-wear</td>\n",
       "      <td>fashion/ready-to-wear</td>\n",
       "      <td>England</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   backers_count  converted_pledged_amount country created_at currency  \\\n",
       "0              6                      1847      GB 2015-04-03      GBP   \n",
       "\n",
       "  current_currency       deadline  disable_communication  fx_rate    goal  \\\n",
       "0              USD  1430956800000                  False  1.30399  7000.0   \n",
       "\n",
       "   ...  spotlight  staff_pick     state state_changed_at  static_usd_rate  \\\n",
       "0  ...      False       False  canceled       2015-04-19         1.491538   \n",
       "\n",
       "                                                urls  usd_pledged  \\\n",
       "0  {\"web\":{\"project\":\"https://www.kickstarter.com...  1842.049134   \n",
       "\n",
       "        cat_name               cat_slug  loc_state  \n",
       "0  Ready-to-wear  fashion/ready-to-wear    England  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deadline's date appears to have changed format, as such it is returned to desired format\n",
    "df['deadline'] = pd.to_datetime(df['deadline'], unit='ms')\n",
    "df['deadline'] = pd.to_datetime(df['deadline'].dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get names of indexes with states of canceled, live and suspended and remove them as they are\n",
    "# not an indication of success or failure. Cancellation and suspension could be for various reasons other\n",
    "# than failure.\n",
    "indexNames = df[(df['state'] == 'canceled')|(df['state'] == 'live')|(df['state'] == 'suspended')].index\n",
    "# Delete these row indexes from dataFrame\n",
    "df.drop(indexNames,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "successful    5174\n",
       "failed        3171\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data target balance\n",
    "df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would like to convert countries into continents \n",
    "df['country'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if df.loc[index,'country'] == 'US' or df.loc[index,'country'] == 'CA':\n",
    "        df.loc[index,'country'] = 'NAmerica'\n",
    "    elif df.loc[index,'country'] == 'NZ' or df.loc[index,'country'] == 'AU':\n",
    "        df.loc[index,'country'] = 'Aus'\n",
    "    elif df.loc[index,'country'] == 'JP' or df.loc[index,'country'] == 'CH' or df.loc[index,'country'] == 'HK' or df.loc[index,'country'] == 'SG':\n",
    "        df.loc[index,'country'] = 'Aisa'\n",
    "    elif df.loc[index,'country'] == 'MX':\n",
    "        df.loc[index,'country'] = 'SAmerica'\n",
    "    else:\n",
    "        df.loc[index,'country'] = 'Euro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_allowed feature is the time between creating the kickstarter project and ending it in days\n",
    "df['time_allowed'] = df['state_changed_at']-df['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which is then converted into an integer\n",
    "df.time_allowed = df.time_allowed.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of funding pledged/goal is used to normalise the date and replace 'goal' and 'pledge' amount\n",
    "df['pledge/goal'] = (df['converted_pledged_amount']/df['goal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another ration of goal and time_allowed is attempted\n",
    "df['goal/time_allowed'] = (df['goal']/df['time_allowed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below I am inspecting the data to gauge presence of outliers and understand what is the range I am looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.goal.describe();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.time_allowed.describe();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pledge/goal'].describe();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pledge/goal'].value_counts();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_index = list((df[df['pledge/goal'] > 4.5].index)|((df[df['time_allowed'] >= 2000].index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outliers_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(outliers_index,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8304, 28)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     8304.000000\n",
       "mean         3.960467\n",
       "std        142.233836\n",
       "min          0.000000\n",
       "25%          0.039493\n",
       "50%          1.028000\n",
       "75%          1.375000\n",
       "max      12575.000000\n",
       "Name: pledge/goal, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pledge/goal'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['pledge/goal'], bins=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning pledge/goal ration into bins\n",
    "bins = [-0.01, 0.5, 1.0 , 1.5, 2.0, 2.5, 3.0, 5.0]\n",
    "bins_pledgeGoal = pd.cut(df['pledge/goal'], bins)\n",
    "bins_pledgeGoal = bins_pledgeGoal.cat.as_ordered()\n",
    "df[\"pledge/goal\"]=bins_pledgeGoal\n",
    "df[\"pledge/goal\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick contineous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = df[['time_allowed','goal', 'backers_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_c.to_csv(r'csv_\\X_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick categotical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d = pd.get_dummies(df[['cat_slug', 'staff_pick', 'pledge/goal']], drop_first=True, prefix_sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename 'pledge/goal_...' as they pose issues when running models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d.rename(columns={'pledge/goal_(0.5, 1.0]': 'p/g_low','pledge/goal_(1.0, 1.5]': 'p/g_high', 'pledge/goal_(1.5, 2.0]': 'p/g_vhigh',  'pledge/goal_(2.0, 2.5]': 'p/g_shigh', 'pledge/goal_(2.0, 2.5]':'p/g_sdhigh', 'pledge/goal_(2.5, 3.0]': 'p/g_uhigh', 'pledge/goal_(3.0, 5.0]': 'p/g_extraOrdinarlyHigh' }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_d.to_csv(r'csv_\\X_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join X_c and X_d Features\n",
    "X = pd.concat([X_c,X_d],1)\n",
    "X;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target to binary\n",
    "y = pd.get_dummies(df[['state']], drop_first=True)\n",
    "y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.to_csv(r'csv_\\y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot pair relationships\n",
    "sns.pairplot(pd.concat([X_c,y],1), diag_kind=\"kde\", height=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.concat([X_c,y],1)).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following models will be used:\n",
    "   - Logistic regression\n",
    "   - Random forest\n",
    "   - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split, Train, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_vanilla = RandomForestClassifier()\n",
    "forest_vanilla.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train_v_rf = forest_vanilla.predict(X_train)\n",
    "y_hat_test_v_rf = forest_vanilla.predict(X_test)\n",
    "accuracy_score(y_train, y_hat_train_v_rf), accuracy_score(y_test, y_hat_test_v_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, y_hat_train_v_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_hat_test_v_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying number of max_depth parameter and setting criterion='entropy'\n",
    "array32 = list(range(1,50))\n",
    "clf_list = []\n",
    "for i in array32:\n",
    "    clf_choc = RandomForestClassifier(criterion='entropy', max_depth=i);  #Â Train the classifier using training data \n",
    "    clf_list.append(clf_choc.fit(X_train, y_train))\n",
    "auc_list_train = []\n",
    "for i in clf_list:\n",
    "    y_pred = i.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_pred)\n",
    "    auc_list_train.append(auc(false_positive_rate, true_positive_rate))\n",
    "auc_list_test = []\n",
    "for i in clf_list:\n",
    "    y_pred1 = i.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred1)\n",
    "    auc_list_test.append(auc(false_positive_rate, true_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(array32, auc_list_test, 'b', label='Test AUC')\n",
    "plt.plot(array32, auc_list_train, 'r', label='Train AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying number of min_samples_split parameter and setting criterion='entropy'\n",
    "array1 = list(map(lambda x: round(x,1), np.arange(0.1,1.0,0.1)))\n",
    "clf_list_strawberry = []\n",
    "for i in array1:\n",
    "    clf_strawberry = RandomForestClassifier(criterion='entropy', min_samples_split = i)  #Â Train the classifier using training data \n",
    "    clf_list_strawberry.append(clf_strawberry.fit(X_train, y_train))\n",
    "auc_list_test_strawberry = []\n",
    "for i in clf_list_strawberry:\n",
    "    y_pred = i.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    auc_list_test_strawberry.append(auc(false_positive_rate, true_positive_rate))\n",
    "auc_list_train_strawberry = []\n",
    "for i in clf_list_strawberry:\n",
    "    y_pred1 = i.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_pred1)\n",
    "    auc_list_train_strawberry.append(auc(false_positive_rate, true_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(array1, auc_list_test_strawberry, 'b', label='Test AUC')\n",
    "plt.plot(array1, auc_list_train_strawberry, 'r', label='Train AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree min_samples_split')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying number of min_samples_leaf parameter and setting criterion='entropy'\n",
    "array05 = list(map(lambda x: round(x,1), np.arange(0.1,0.5,0.1)))\n",
    "clf_list_cookiencream = []\n",
    "for i in array05:\n",
    "    clf_cookiencream = RandomForestClassifier(criterion='entropy', min_samples_leaf = i)  #Â Train the classifier using training data \n",
    "    clf_list_cookiencream.append(clf_cookiencream.fit(X_train, y_train))\n",
    "auc_list_test_cookiencream = []\n",
    "for i in clf_list_cookiencream:\n",
    "    y_pred = i.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    auc_list_test_cookiencream.append(auc(false_positive_rate, true_positive_rate))\n",
    "auc_list_train_cookiencream = []\n",
    "for i in clf_list_cookiencream:\n",
    "    y_pred1 = i.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_pred1)\n",
    "    auc_list_train_cookiencream.append(auc(false_positive_rate, true_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(array05, auc_list_test_cookiencream, 'b', label='Test AUC')\n",
    "plt.plot(array05, auc_list_train_cookiencream, 'r', label='Train AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree min_samples_leaf')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# varying number of max_features parameter and setting criterion='entropy'\n",
    "array200 = range(1,len(X.columns))\n",
    "clf_list_peanutbutter = []\n",
    "for i in array200:\n",
    "    clf_peanutbutter = RandomForestClassifier(criterion='entropy', max_features = i)  #Â Train the classifier using training data \n",
    "    clf_list_peanutbutter.append(clf_peanutbutter.fit(X_train, y_train))\n",
    "auc_list_test_peanutbutter = []\n",
    "for i in clf_list_peanutbutter:\n",
    "    y_pred = i.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    auc_list_test_peanutbutter.append(auc(false_positive_rate, true_positive_rate))\n",
    "auc_list_train_peanutbutter = []\n",
    "for i in clf_list_peanutbutter:\n",
    "    y_pred1 = i.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_pred1)\n",
    "    auc_list_train_peanutbutter.append(auc(false_positive_rate, true_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(array200, auc_list_test_peanutbutter, 'b', label='Test AUC')\n",
    "plt.plot(array200, auc_list_train_peanutbutter, 'r', label='Train AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree max_features')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_rf = RandomForestClassifier(criterion='entropy', max_depth=12, min_samples_split =0.2, max_features = 22)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_hat_rf_train = clf_rf.predict(X_train)\n",
    "y_hat_rf_test = clf_rf.predict(X_test)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_hat_rf_test)\n",
    "auc(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, y_hat_rf_train), accuracy_score(y_test, y_hat_rf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data achieves same accuracy as Vanilla\n",
    "print(confusion_matrix(y_test, y_hat_test_v_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_vanilla = LogisticRegression()\n",
    "model_log = logreg_vanilla.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score_lr_v = logreg_vanilla.decision_function(X_test)\n",
    "y_hat_test_lr_v = logreg_vanilla.predict(X_test)\n",
    "y_hat_train_lr_v = logreg_vanilla.predict(X_train)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score_lr_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "print(confusion_matrix(y_train, y_hat_train_lr_v))\n",
    "print(confusion_matrix(y_test, y_hat_test_lr_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy enhanced slightly when compared to RandomForest\n",
    "accuracy_score(y_test, y_hat_test_lr_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#Seaborns Beautiful Styling\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change C parameter\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100]\n",
    "clf_list = []\n",
    "for i in C_param_range:\n",
    "    clf_choc = LogisticRegression(C=i);  #Â Train the classifier using training data \n",
    "    clf_list.append(clf_choc.fit(X_train, y_train))\n",
    "auc_list_train_lr_choc = []\n",
    "for i in clf_list:\n",
    "    y_pred = i.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_pred)\n",
    "    auc_list_train_lr_choc.append(auc(false_positive_rate, true_positive_rate))\n",
    "auc_list_test_lr_choc = []\n",
    "for i in clf_list:\n",
    "    y_pred1 = i.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred1)\n",
    "    auc_list_test_lr_choc.append(auc(false_positive_rate, true_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(C_param_range, auc_list_test_lr_choc, 'b', label='Test AUC')\n",
    "plt.plot(C_param_range, auc_list_train_lr_choc, 'r', label='Train AUC')\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('C Parameter')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change penlty and corresponding solver type\n",
    "penalty_list = {'l1':'liblinear', 'l2':'liblinear', 'none':'lbfgs'}\n",
    "clf_list_strawberry = []\n",
    "for k,v in penalty_list.items():\n",
    "    print(k,v)\n",
    "    clf_strawberry = LogisticRegression(penalty = k,solver=v)  #Â Train the classifier using training data \n",
    "    clf_list_strawberry.append(clf_strawberry.fit(X_train, y_train))\n",
    "auc_list_test_strawberry_lr = []\n",
    "for i in clf_list_strawberry:\n",
    "    y_pred = i.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    auc_list_test_strawberry_lr.append(auc(false_positive_rate, true_positive_rate))\n",
    "auc_list_train_strawberry_lr = []\n",
    "for i in clf_list_strawberry:\n",
    "    y_pred1 = i.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_pred1)\n",
    "    auc_list_train_strawberry_lr.append(auc(false_positive_rate, true_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_list_test_strawberry_lr, auc_list_train_strawberry_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set width of bar\n",
    "plt.figure(figsize=(12,6))\n",
    "barWidth = 0.25\n",
    " \n",
    "# set height of bar\n",
    "bars1 = auc_list_test_strawberry_lr\n",
    "bars2 = auc_list_train_strawberry_lr\n",
    " \n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(r1, bars1, width=barWidth, edgecolor='white', label='var1')\n",
    "plt.bar(r2, bars2, color='#2d7f5e', width=barWidth, edgecolor='white', label='var2')\n",
    " \n",
    "# Add xticks on the middle of the group bars\n",
    "plt.xlabel('group', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], list(penalty_list.keys()))\n",
    " \n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little difference - keep as Vanilla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost = xgb.XGBClassifier()\n",
    "XGBoost.fit(X_train, y_train)\n",
    "y_hat_train = XGBoost.predict(X_train)\n",
    "y_hat_test = XGBoost.predict(X_test)\n",
    "training_accuracy = accuracy_score(y_train, y_hat_train)\n",
    "val_accuracy = accuracy_score(y_test, y_hat_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training Accuracy: {:.4}%\".format(training_accuracy * 100))\n",
    "print(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"learning_rate\": [0.1],\n",
    "    'max_depth': [6],\n",
    "    'min_child_weight': [10],\n",
    "    'subsample': [ 0.7],\n",
    "    'n_estimators': [5, 30, 100, 250],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf = GridSearchCV(XGBoost, param_grid, scoring='accuracy', cv=None, n_jobs=1)\n",
    "grid_clf.fit(X, y)\n",
    "\n",
    "best_parameters = grid_clf.best_params_\n",
    "\n",
    "print(\"Grid Search found the following optimal parameters: \")\n",
    "for param_name in sorted(best_parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "y_hat_train = grid_clf.predict(X_train)\n",
    "y_hat_test = grid_clf.predict(X_test)\n",
    "training_accuracy = accuracy_score(y_train, y_hat_train)\n",
    "val_accuracy = accuracy_score(y_test, y_hat_test)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training Accuracy: {:.4}%\".format(training_accuracy * 100))\n",
    "print(\"Validation accuracy: {:.4}%\".format(val_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search found the following optimal parameters: \n",
    "# learning_rate: 0.1\n",
    "# max_depth: 6\n",
    "# min_child_weight: 10\n",
    "# n_estimators: 30\n",
    "# subsample: 0.7\n",
    "\n",
    "# Training Accuracy: 96.55%\n",
    "# Validation accuracy: 96.12%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All work very well and pretty much predict with the same accuracy. Next I will try PCA for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect and explain\n",
    "# label figures "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
